{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\n# Filter out the specific UserWarnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"unable to load libtensorflow_io_plugins.so\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"file system plugins are not loaded\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hugging Face library\nfrom transformers import AutoTokenizer, TFAutoModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hugging Face library\nfrom datasets import Dataset, DatasetDict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy metrics from Scikit-Learn\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras.optimizers import AdamW\n\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\n\nfrom tensorflow.keras.layers import Input, Dense, GlobalMaxPooling1D\n\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Load Datasets","metadata":{}},{"cell_type":"code","source":"# Create a function to import the data from csv format\ndef load_data(file_path):\n    return pd.read_csv(file_path, header=None, delimiter='\\t', names=['sentiment', 'text'])\n\n\ntrain_path = '/kaggle/input/sentiment/train_bal_vdg_27_11.tsv'\ntest_path = '/kaggle/input/sentiment/test_bal_vdg_27_11.tsv'\nval_path = '/kaggle/input/sentiment/valid_bal_vdg_27_11.tsv'\n\ndf_train = load_data(train_path)\ndf_test = load_data(test_path)\ndf_val = load_data(val_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since I'm gonna use the sparse categorical cross entropy loss, I map the labels to integers\nencoded_dict = {'NEG':0, 'NEU':1, 'POS':2}\n\ndf_train['label'] = df_train['sentiment'].apply(lambda x: encoded_dict[x])\ndf_test['label'] = df_test['sentiment'].apply(lambda x: encoded_dict[x])\ndf_val['label'] = df_val['sentiment'].apply(lambda x: encoded_dict[x])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To get an idea of the data\npd.set_option('display.max_colwidth', 150)\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Both these functions can have as input a single label/id or a list of them\n\ndef label2id(label):\n    if isinstance(label, list):\n        return [encoded_dict[label] for label in label]\n    else:\n        return encoded_dict[label]\n\ndef id2label(id):\n    encoded_dict_inv = {v: k for k, v in encoded_dict.items()}\n    \n    if isinstance(id, list):\n        return [encoded_dict_inv[i] for i in id]\n    else:\n        return encoded_dict_inv[id]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I'm combining the pandas dataframe to the dataset dictionary of Hugging Face\n\ntrain_dataset = Dataset.from_pandas(df_train)\ntest_dataset = Dataset.from_pandas(df_test)\nval_dataset = Dataset.from_pandas(df_val)\n\n# Create the DatasetDict\ndataset = DatasetDict({'train': train_dataset, 'test': test_dataset, 'validation': val_dataset})\n\nprint(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing duplicates\n\n# Initialize a dictionary to store updated datasets\nupdated_datasets = {}\n\n# Check for and remove duplicates in each split\nfor split in dataset.keys():\n    split_data = dataset[split]\n    \n    # Access the 'text' column within the list\n    text_column = split_data['text']\n    \n    # Initialize a set to track unique texts\n    unique_texts = set()\n    \n    # Initialize lists to store the filtered data\n    filtered_text = []\n    \n    # Iterate through the 'text' column and filter duplicates\n    for text in text_column:\n        if text not in unique_texts:\n            unique_texts.add(text)\n            filtered_text.append(text)\n    \n    # Create a new Dataset object with the filtered data\n    updated_datasets[split] = split_data.select(list(range(len(filtered_text))))\n    \n    # Print the number of removed duplicates\n    duplicate_count = len(text_column) - len(filtered_text)\n    print(f\"Duplicates removed in {split} split: {duplicate_count}\\n\")\n\n# Update the dataset dictionary with the filtered datasets\ndataset.update(updated_datasets)\n\n# Print the updated dataset information\nfor split in dataset.keys():\n    split_data = dataset[split]\n    print(f\"{split}: {len(split_data['text'])} rows\")\n\nprint(dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Preprocess data","metadata":{}},{"cell_type":"code","source":"bert = TFAutoModel.from_pretrained('Twitter/twhin-bert-base', from_pt=True)\ntokenizer = AutoTokenizer.from_pretrained('Twitter/twhin-bert-base')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 256\n\ndef tokenize_text(dataset):\n    return tokenizer(\n        text=dataset['text'],\n        add_special_tokens=True,\n        return_token_type_ids=False,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_tensors='tf',\n        verbose=True\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_dataset = dataset.map(tokenize_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_dataset = encoded_dataset.remove_columns(['sentiment','text'])\n\nencoded_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(encoded_dataset, data_type):\n    input_ids = np.array(encoded_dataset[data_type]['input_ids'])\n    input_ids = np.squeeze(input_ids, axis=1)\n\n    attention_mask = np.array(encoded_dataset[data_type]['attention_mask'])\n    attention_mask = np.squeeze(attention_mask, axis=1)\n\n    label = np.array(encoded_dataset[data_type]['label'])\n\n    return input_ids, attention_mask, label\n\ndef main_processing(encoded_dataset):\n    input_ids_train, attention_mask_train, label_train = preprocess_data(encoded_dataset, 'train')\n    input_ids_val, attention_mask_val, label_val = preprocess_data(encoded_dataset, 'validation')\n    input_ids_test, attention_mask_test, label_test = preprocess_data(encoded_dataset, 'test')\n\n    return input_ids_train, attention_mask_train, label_train, input_ids_val, attention_mask_val, label_val, input_ids_test, attention_mask_test, label_test\n\n# Usage\ninput_ids_train, attention_mask_train, label_train, input_ids_val, attention_mask_val, label_val, input_ids_test, attention_mask_test, label_test = main_processing(encoded_dataset)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Defining the model","metadata":{}},{"cell_type":"code","source":"input_ids = Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n\nembeddings = bert.bert(input_ids, attention_mask = input_mask)[0]\nout = GlobalMaxPooling1D(name=\"GlobalMaxPooling1d\")(embeddings)\nout = Dense(128, activation='relu',name=\"Dense_relu\")(out)\n\n\ny = Dense(3, activation='softmax',name=\"Dense_softmax\")(out)\n    \nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\nmodel.layers[2].trainable = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, layer in enumerate(model.layers):\n    print(f\"Layer {i}: {layer.name}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(\n    learning_rate=1e-5,\n    epsilon=1e-08,\n    weight_decay=0.001,\n    name=\"AdamW\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scheduler(epoch,lr):\n    if epoch <2:\n        return lr\n    else:\n        return lr*tf.math.exp(-0.1)\n    \nlr_scheduler = LearningRateScheduler(scheduler)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = SparseCategoricalCrossentropy(\n    from_logits=False,\n    ignore_class=None,\n    reduction=\"auto\",\n    name=\"sparse_categorical_crossentropy\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stop = EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0,\n    patience=4,\n    verbose=0,\n    mode=\"auto\",\n    baseline=None,\n    restore_best_weights=True,\n    start_from_epoch=0,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer=optimizer,\n    loss=loss,\n    metrics=['sparse_categorical_accuracy']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output shape is shown as \"(None, ...)\" in the layer summary you provided because the specific batch size dimension is not fixed in the layer summary. In many deep learning frameworks, including TensorFlow, Keras, and others, when you define a model, you typically leave the batch size dimension as \"None\" in the layer summary. The \"None\" here indicates that the batch size is not specified at the model definition stage and will be determined dynamically during training or inference based on the input data.","metadata":{}},{"cell_type":"markdown","source":"input_ids and attention_mask:\n\nShape: (None, 256)\nExplanation: These input layers are typically used for processing sequences, such as text data. The (None, 256) shape means that the model expects input sequences with a maximum length of 256 tokens, and the batch size can vary (indicated by \"None\").\n\nbert (TFBertMainLayer):\n\nOutput Shape: (None, 256, 768)\nExplanation: This is the output shape of the BERT model. It processes input sequences and produces embeddings for each token in the sequence. The first dimension \"None\" represents the batch size, the second dimension \"256\" represents the sequence length, and the third dimension \"768\" represents the size of the hidden representation for each token.\n\nglobal_max_pooling1d (GlobalMaxPooling1D):\n\nOutput Shape: (None, 768)\nExplanation: This layer performs global max-pooling over the token embeddings generated by BERT. It takes the maximum value across the sequence length dimension (256) for each of the 768 hidden units, resulting in a fixed-size representation for each input example. The \"None\" batch dimension remains unspecified.\n\ndense (Dense):\n\nOutput Shape: (None, 128)\nExplanation: This is a fully connected (dense) layer with 128 output units. It takes the output from the global max-pooling layer and transforms it into a lower-dimensional space. The \"None\" batch dimension indicates variable batch size.\n\ndropout_37 (Dropout):\n\nOutput Shape: (None, 128)\nExplanation: Dropout is a regularization technique where a fraction of input units is randomly set to zero during each update, helping to prevent overfitting. The \"None\" batch dimension remains unspecified.\n\ndense_1 (Dense):\n\nOutput Shape: (None, 32)\nExplanation: This is another fully connected layer with 32 output units. It further reduces the dimensionality of the data. The \"None\" batch dimension indicates variable batch size.\n\ndense_2 (Dense):\n\nOutput Shape: (None, 3)\nExplanation: This is the final dense layer with 3 output units. It produces the final predictions or scores for a classification task with 3 classes. The \"None\" batch dimension remains unspecified.","metadata":{}},{"cell_type":"markdown","source":"# 4. Training","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n    x = {'input_ids':input_ids_train, 'attention_mask':attention_mask_train},\n    y = label_train,\n    validation_data = ({'input_ids':input_ids_val, 'attention_mask':attention_mask_val},\n                      (label_val)),\n    epochs=15,\n    batch_size=16,\n    callbacks=[early_stop, lr_scheduler]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='train_loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Metrics","metadata":{}},{"cell_type":"code","source":"predicted = model.predict({'input_ids': input_ids_test, 'attention_mask': attention_mask_test})\npredicted_labels = np.argmax(predicted, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_labels = predicted_labels.tolist()\npredicted_labels = id2label(predicted_labels)\npredicted_labels[0:7]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_test = label_test.tolist()\nlabel_test = id2label(label_test)\nlabel_test[0:7]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['test']['text'][0:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(label_test, predicted_labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(label_test, predicted_labels, labels=['NEG', 'NEU', 'POS'])\n\n# Create a heatmap for the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['NEG', 'NEU', 'POS'], yticklabels=['NEG', 'NEU', 'POS'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.1 Accuracy Score","metadata":{}},{"cell_type":"code","source":"accuracy = accuracy_score(label_test, predicted_labels) # (TP+TN)/P+N i.e total number of corrected classified tweet over total number of tweets\n\nprint(accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Preision Score","metadata":{}},{"cell_type":"code","source":"precision = precision_score(label_test, predicted_labels,average=None, labels=['NEG','NEU','POS']) # TP/(TP+FP) i.e if predicted a certain class, which is the probability of being really that class?\n\nprint(precision)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Recall (sensitivity) Score","metadata":{}},{"cell_type":"code","source":"recall = recall_score(label_test, predicted_labels,average=None, labels=['NEG','NEU','POS']) # TP/(TP+FN) i.e the ability of the estimator to predict all the tweets of a given class\n\nprint(recall)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.4 F1 Score","metadata":{}},{"cell_type":"code","source":"f1score = f1_score(label_test, predicted_labels,average=None, labels=['NEG','NEU','POS']) # 2*(precision*recall)/(precision+recall)\n\nprint(f1score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Push To Hub","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import push_to_hub_keras\n\npush_to_hub_keras(model, 'FedeBerto/Griffith-Sentiment')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import from_pretrained_keras\n\nmodel = from_pretrained_keras('FedeBerto/Griffith-Sentiment')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
