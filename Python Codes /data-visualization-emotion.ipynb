{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:04:51.902913Z","iopub.execute_input":"2023-09-18T12:04:51.903310Z","iopub.status.idle":"2023-09-18T12:04:52.362200Z","shell.execute_reply.started":"2023-09-18T12:04:51.903279Z","shell.execute_reply":"2023-09-18T12:04:52.360737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\n# Filter out the specific UserWarnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"unable to load libtensorflow_io_plugins.so\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"file system plugins are not loaded\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:05:09.068694Z","iopub.execute_input":"2023-09-18T12:05:09.069236Z","iopub.status.idle":"2023-09-18T12:05:09.076995Z","shell.execute_reply.started":"2023-09-18T12:05:09.069204Z","shell.execute_reply":"2023-09-18T12:05:09.075461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hugging Face library\nfrom transformers import AutoTokenizer, TFAutoModel","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:05:10.783196Z","iopub.execute_input":"2023-09-18T12:05:10.783615Z","iopub.status.idle":"2023-09-18T12:05:13.294077Z","shell.execute_reply.started":"2023-09-18T12:05:10.783571Z","shell.execute_reply":"2023-09-18T12:05:13.292769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hugging Face library\nfrom datasets import Dataset, DatasetDict","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:05:13.296287Z","iopub.execute_input":"2023-09-18T12:05:13.296835Z","iopub.status.idle":"2023-09-18T12:05:14.137643Z","shell.execute_reply.started":"2023-09-18T12:05:13.296800Z","shell.execute_reply":"2023-09-18T12:05:14.136528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:05:14.139229Z","iopub.execute_input":"2023-09-18T12:05:14.139815Z","iopub.status.idle":"2023-09-18T12:05:15.390842Z","shell.execute_reply.started":"2023-09-18T12:05:14.139782Z","shell.execute_reply":"2023-09-18T12:05:15.389529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Load Datasets","metadata":{}},{"cell_type":"code","source":"# Create a function to import the data from csv format\ndef load_data(file_path):\n    return pd.read_csv(file_path, header=None, delimiter='\\t', names=['emotion', 'text'])\n\n\ntrain_path = '/kaggle/input/emotion/train-emotion-all.tsv'\ntest_path = '/kaggle/input/emotion/test-emotion-all.tsv'\nval_path = '/kaggle/input/emotion/valid-emotion-all.tsv'\n\ndf_train = load_data(train_path)\ndf_test = load_data(test_path)\ndf_val = load_data(val_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:08:25.858446Z","iopub.execute_input":"2023-09-18T12:08:25.858895Z","iopub.status.idle":"2023-09-18T12:08:25.884381Z","shell.execute_reply.started":"2023-09-18T12:08:25.858862Z","shell.execute_reply":"2023-09-18T12:08:25.883330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To get an idea of the data\npd.set_option('display.max_colwidth', 150)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:08:26.321590Z","iopub.execute_input":"2023-09-18T12:08:26.322190Z","iopub.status.idle":"2023-09-18T12:08:26.333829Z","shell.execute_reply.started":"2023-09-18T12:08:26.322151Z","shell.execute_reply":"2023-09-18T12:08:26.332804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I'm combining the pandas dataframe to the dataset dictionary of Hugging Face\n\ntrain_dataset = Dataset.from_pandas(df_train)\ntest_dataset = Dataset.from_pandas(df_test)\nval_dataset = Dataset.from_pandas(df_val)\n\n# Create the DatasetDict\ndataset = DatasetDict({'train': train_dataset, 'test': test_dataset, 'validation': val_dataset})\n\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:08:26.668072Z","iopub.execute_input":"2023-09-18T12:08:26.668472Z","iopub.status.idle":"2023-09-18T12:08:26.688048Z","shell.execute_reply.started":"2023-09-18T12:08:26.668433Z","shell.execute_reply":"2023-09-18T12:08:26.686635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.5 Check Duplicates","metadata":{}},{"cell_type":"code","source":"# Initialize a dictionary to store updated datasets\nupdated_datasets = {}\n\n# Check for and remove duplicates in each split\nfor split in dataset.keys():\n    split_data = dataset[split]\n    \n    # Access the 'text' column within the list\n    text_column = split_data['text']\n    \n    # Initialize a set to track unique texts\n    unique_texts = set()\n    \n    # Initialize lists to store the filtered data\n    filtered_text = []\n    \n    # Iterate through the 'text' column and filter duplicates\n    for text in text_column:\n        if text not in unique_texts:\n            unique_texts.add(text)\n            filtered_text.append(text)\n    \n    # Create a new Dataset object with the filtered data\n    updated_datasets[split] = split_data.select(list(range(len(filtered_text))))\n    \n    # Print the number of removed duplicates\n    duplicate_count = len(text_column) - len(filtered_text)\n    print(f\"Duplicates removed in {split} split: {duplicate_count}\\n\")\n\n# Update the dataset dictionary with the filtered datasets\ndataset.update(updated_datasets)\n\n# Print the updated dataset information\nfor split in dataset.keys():\n    split_data = dataset[split]\n    print(f\"{split}: {len(split_data['text'])} rows\")\n\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:08:27.352641Z","iopub.execute_input":"2023-09-18T12:08:27.353269Z","iopub.status.idle":"2023-09-18T12:08:27.415260Z","shell.execute_reply.started":"2023-09-18T12:08:27.353239Z","shell.execute_reply":"2023-09-18T12:08:27.414010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Sentiment Distribution","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\n# Initialize dictionaries to store sentiment counts and categories for each split\nemotion_counts = {}\nemotion_categories = {}\n\n# Loop through each split\nfor split in dataset.keys():\n    split_data = dataset[split]\n    \n    # Extract emotion data from the 'emotion' column within the list\n    emotion_data = [item for item in split_data['emotion']]\n    \n    # Calculate sentiment counts for the current split using Counter\n    emotion_count = Counter(emotion_data)\n    emotion_counts[split] = emotion_count\n    \n    # Get unique emotion categories for the current split\n    emotion_category = list(emotion_count.keys())\n    emotion_categories[split] = emotion_category\n\n# Combine all unique sentiment categories across all splits and sort them\nall_emotion_categories = sorted(set().union(*emotion_categories.values()))\n\n# Create subplots for each split\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n#max_y_value = 0\n# Loop through each split and plot the sentiment counts\nfor i, split in enumerate(dataset.keys()):\n    x = np.array(all_emotion_categories)\n    y = np.array([emotion_counts[split].get(category, 0) for category in x])\n    axs[i].bar(x, y)\n    \n    \n    \n    axs[i].set_title(f\"Emotion Distribution ({split} split)\")\n    axs[i].set_xlabel(\"Emotion category\")\n    axs[i].set_ylabel(\"Number of tweets\")\n    \n    axs[i].tick_params(axis='x', labelsize=7)\n    #max_y_value = max(max_y_value, max(y))\n    \n# Set a common y-axis range for all subplots\n#for i in range(3):\n    #axs[i].set_ylim(0, max_y_value)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:19:24.382518Z","iopub.execute_input":"2023-09-18T12:19:24.382996Z","iopub.status.idle":"2023-09-18T12:19:25.241013Z","shell.execute_reply.started":"2023-09-18T12:19:24.382962Z","shell.execute_reply":"2023-09-18T12:19:25.239539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. WordCloud","metadata":{}},{"cell_type":"code","source":"!python -m spacy download it_core_news_md","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:16:00.598344Z","iopub.execute_input":"2023-09-18T12:16:00.598803Z","iopub.status.idle":"2023-09-18T12:16:36.251535Z","shell.execute_reply.started":"2023-09-18T12:16:00.598767Z","shell.execute_reply":"2023-09-18T12:16:36.250392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\n\n# Load the Italian language model\nnlp = spacy.load(\"it_core_news_md\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:16:36.254250Z","iopub.execute_input":"2023-09-18T12:16:36.254687Z","iopub.status.idle":"2023-09-18T12:16:44.754096Z","shell.execute_reply.started":"2023-09-18T12:16:36.254649Z","shell.execute_reply":"2023-09-18T12:16:44.752942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nitalian_stopwords = set(stopwords.words('italian'))\n\nfrom nltk.tokenize import word_tokenize \n\n# Define a function to preprocess text\ndef preprocess_text(text):\n    # Remove punctuation, URLs, and user mentions\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'@\\w+', '', text)\n    \n    # Process text using spaCy\n    doc = nlp(text)\n    \n    # Filter out only nouns and lemmatize them\n    lemmatized_nouns = [token.lemma_ for token in doc  if token.pos_ == 'NOUN']\n    \n    # Remove stopwords\n    lemmatized_nouns = [word for word in lemmatized_nouns if word.lower() not in italian_stopwords]\n    \n    # Join the filtered and lemmatized nouns into a string\n    text = ' '.join(lemmatized_nouns)\n    \n    return text\n\n\n\n\n\ndef preprocess_dataset(dataset):\n    dataset['text'] = preprocess_text(dataset['text'])\n    return dataset\n\ndataset = dataset.map(preprocess_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:19:31.872867Z","iopub.execute_input":"2023-09-18T12:19:31.873274Z","iopub.status.idle":"2023-09-18T12:19:59.904311Z","shell.execute_reply.started":"2023-09-18T12:19:31.873244Z","shell.execute_reply":"2023-09-18T12:19:59.903041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:20:24.240177Z","iopub.execute_input":"2023-09-18T12:20:24.240729Z","iopub.status.idle":"2023-09-18T12:20:24.309891Z","shell.execute_reply.started":"2023-09-18T12:20:24.240683Z","shell.execute_reply":"2023-09-18T12:20:24.308616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a WordCloud object\n\n# Combine the text from 'train', 'test', and 'validation' splits\ncombined_text = []\n\nfor split in ['train', 'test', 'validation']:\n    combined_text.extend([text for text in dataset[split]['text']])\n\n# Concatenate the combined text into a single string\ntext = ' '.join(combined_text)\n\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n\n# Display the word cloud using matplotlib\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")  # Turn off axis labels\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:20:30.923110Z","iopub.execute_input":"2023-09-18T12:20:30.923846Z","iopub.status.idle":"2023-09-18T12:20:32.314823Z","shell.execute_reply.started":"2023-09-18T12:20:30.923810Z","shell.execute_reply":"2023-09-18T12:20:32.313836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Checking word distribution among classes","metadata":{}},{"cell_type":"code","source":"# Create empty lists for each class\ntristezza_text = []\ngioia_text = []\namore_text = []\nrabbia_text = []\npaura_text = []\nsorpresa_text = []\nneutra_text = []\n\nfor split in dataset.keys():\n    for text, emotion in zip(dataset[split]['text'], dataset[split]['emotion']):\n        # Check the emotion label and append the text to the corresponding list\n        if emotion == 'TRISTEZZA':\n            tristezza_text.append(text)\n        elif emotion == 'GIOIA':\n            gioia_text.append(text)\n        elif emotion == 'AMORE':\n            amore_text.append(text)\n        elif emotion == 'RABBIA':\n            rabbia_text.append(text)\n        elif emotion == 'PAURA':\n            paura_text.append(text)\n        elif emotion == 'SORPRESA':\n            sorpresa_text.append(text)\n        elif emotion == 'NEUTRA':\n            neutra_text.append(text)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:24:12.307910Z","iopub.execute_input":"2023-09-18T12:24:12.308367Z","iopub.status.idle":"2023-09-18T12:24:12.330091Z","shell.execute_reply.started":"2023-09-18T12:24:12.308328Z","shell.execute_reply":"2023-09-18T12:24:12.328874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All words contained in \"category tweets\"\ntristezza_nouns = []\ngioia_nouns = []\namore_nouns = []\nrabbia_nouns = []\npaura_nouns = []\nsorpresa_nouns = []\nneutra_nouns = []\n\n# Process text based on emotions\nfor split in dataset.keys():\n    for text, emotion in zip(dataset[split]['text'], dataset[split]['emotion']):\n        doc = nlp(text)\n        nouns = [token.text for token in doc if token.pos_ == 'NOUN']\n        \n        if emotion == 'TRISTEZZA':\n            tristezza_nouns.extend(nouns)\n        elif emotion == 'GIOIA':\n            gioia_nouns.extend(nouns)\n        elif emotion == 'AMORE':\n            amore_nouns.extend(nouns)\n        elif emotion == 'RABBIA':\n            rabbia_nouns.extend(nouns)\n        elif emotion == 'PAURA':\n            paura_nouns.extend(nouns)\n        elif emotion == 'SORPRESA':\n            sorpresa_nouns.extend(nouns)\n        elif emotion == 'NEUTRA':\n            neutra_nouns.extend(nouns)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:24:12.920488Z","iopub.execute_input":"2023-09-18T12:24:12.921203Z","iopub.status.idle":"2023-09-18T12:24:31.056174Z","shell.execute_reply.started":"2023-09-18T12:24:12.921166Z","shell.execute_reply":"2023-09-18T12:24:31.054843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.probability import FreqDist\n\n\nfdist_tristezza = FreqDist(tristezza_nouns)\nfdist_gioia = FreqDist(gioia_nouns)\nfdist_amore = FreqDist(amore_nouns)\nfdist_rabbia = FreqDist(rabbia_nouns)\nfdist_paura = FreqDist(paura_nouns)\nfdist_sorpresa = FreqDist(sorpresa_nouns)\nfdist_neutra = FreqDist(neutra_nouns)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:24:31.058012Z","iopub.execute_input":"2023-09-18T12:24:31.058374Z","iopub.status.idle":"2023-09-18T12:24:31.084176Z","shell.execute_reply.started":"2023-09-18T12:24:31.058343Z","shell.execute_reply":"2023-09-18T12:24:31.074001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the most common words for each emotion category\ntop_words_tristezza = fdist_tristezza.most_common(15)\ntop_words_gioia = fdist_gioia.most_common(15)\ntop_words_amore = fdist_amore.most_common(15)\ntop_words_rabbia = fdist_rabbia.most_common(15)\ntop_words_paura = fdist_paura.most_common(15)\ntop_words_sorpresa = fdist_sorpresa.most_common(15)\ntop_words_neutra = fdist_neutra.most_common(15)\n\n# Unzip the top words and frequencies for each emotion\ntop_words_tristezza, frequencies_tristezza = zip(*top_words_tristezza)\ntop_words_gioia, frequencies_gioia = zip(*top_words_gioia)\ntop_words_amore, frequencies_amore = zip(*top_words_amore)\ntop_words_rabbia, frequencies_rabbia = zip(*top_words_rabbia)\ntop_words_paura, frequencies_paura = zip(*top_words_paura)\ntop_words_sorpresa, frequencies_sorpresa = zip(*top_words_sorpresa)\ntop_words_neutra, frequencies_neutra = zip(*top_words_neutra)\n\n# Create subplots\nfig, axes = plt.subplots(7, 1, figsize=(8, 30), sharex=True)\n\n# Plot for Tristezza words\naxes[0].barh(top_words_tristezza, frequencies_tristezza)\naxes[0].set_xlabel('Frequency')\naxes[0].set_ylabel('Words')\naxes[0].set_title('Top 15 Tristezza Words')\naxes[0].invert_yaxis()\n\n# Plot for Gioia words\naxes[1].barh(top_words_gioia, frequencies_gioia)\naxes[1].set_xlabel('Frequency')\naxes[1].set_ylabel('Words')\naxes[1].set_title('Top 15 Gioia Words')\naxes[1].invert_yaxis()\n\n# Plot for Amore words\naxes[2].barh(top_words_amore, frequencies_amore)\naxes[2].set_xlabel('Frequency')\naxes[2].set_ylabel('Words')\naxes[2].set_title('Top 15 Amore Words')\naxes[2].invert_yaxis()\n\n# Plot for Rabbia words\naxes[3].barh(top_words_rabbia, frequencies_rabbia)\naxes[3].set_xlabel('Frequency')\naxes[3].set_ylabel('Words')\naxes[3].set_title('Top 15 Rabbia Words')\naxes[3].invert_yaxis()\n\n# Plot for Paura words\naxes[4].barh(top_words_paura, frequencies_paura)\naxes[4].set_xlabel('Frequency')\naxes[4].set_ylabel('Words')\naxes[4].set_title('Top 15 Paura Words')\naxes[4].invert_yaxis()\n\n# Plot for Sorpresa words\naxes[5].barh(top_words_sorpresa, frequencies_sorpresa)\naxes[5].set_xlabel('Frequency')\naxes[5].set_ylabel('Words')\naxes[5].set_title('Top 15 Sorpresa Words')\naxes[5].invert_yaxis()\n\n# Plot for Neutra words\naxes[6].barh(top_words_neutra, frequencies_neutra)\naxes[6].set_xlabel('Frequency')\naxes[6].set_ylabel('Words')\naxes[6].set_title('Top 15 Neutra Words')\naxes[6].invert_yaxis()\n\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:27:27.153287Z","iopub.execute_input":"2023-09-18T12:27:27.153784Z","iopub.status.idle":"2023-09-18T12:27:29.362389Z","shell.execute_reply.started":"2023-09-18T12:27:27.153745Z","shell.execute_reply":"2023-09-18T12:27:29.361088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Similarity Index","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Create a TF-IDF vectorizer and fit it on all tweets\ntfidf_vectorizer = TfidfVectorizer()\nall_tweets = negative + neutral + positive\ntfidf_matrix = tfidf_vectorizer.fit_transform(all_tweets)\n\n# Calculate the cosine similarity between TF-IDF vectors\ncosine_neg_neu = cosine_similarity(tfidf_matrix[:len(negative)], tfidf_matrix[len(negative):len(negative) + len(neutral)])\ncosine_neg_pos = cosine_similarity(tfidf_matrix[:len(negative)], tfidf_matrix[len(negative) + len(neutral):])\ncosine_neu_pos = cosine_similarity(tfidf_matrix[len(negative):len(negative) + len(neutral)], tfidf_matrix[len(negative) + len(neutral):])\n\navg_cosine_neg_neu = np.mean(cosine_neg_neu)\navg_cosine_neg_pos = np.mean(cosine_neg_pos)\navg_cosine_neu_pos = np.mean(cosine_neu_pos)\n\nprint(\"Average Cosine Similarity:\")\nprint(\"NEG vs. NEU:\", avg_cosine_neg_neu)\nprint(\"NEG vs. POS:\", avg_cosine_neg_pos)\nprint(\"NEU vs. POS:\", avg_cosine_neu_pos)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:19:39.098632Z","iopub.execute_input":"2023-09-17T17:19:39.100013Z","iopub.status.idle":"2023-09-17T17:19:39.513357Z","shell.execute_reply.started":"2023-09-17T17:19:39.099956Z","shell.execute_reply":"2023-09-17T17:19:39.512184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
