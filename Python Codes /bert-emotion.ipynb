{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\n# Filter out the specific UserWarnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"unable to load libtensorflow_io_plugins.so\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"file system plugins are not loaded\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, TFAutoModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras.optimizers import AdamW\n\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\n\nfrom tensorflow.keras.layers import Input, Dense, GlobalMaxPooling1D\n\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Load Dataset","metadata":{}},{"cell_type":"code","source":"def load_data(file_path):\n    return pd.read_csv(file_path, header=None, delimiter='\\t', names = ['emotion','text'])\n\ntrain_path = '/kaggle/input/emotion/train-emotion-all.tsv'\ntest_path = '/kaggle/input/emotion/test-emotion-all.tsv'\nval_path = '/kaggle/input/emotion/valid-emotion-all.tsv'\n\ndf_train = load_data(train_path)\ndf_test = load_data(test_path)\ndf_val = load_data(val_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 150)\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Preparing datasets","metadata":{}},{"cell_type":"code","source":"encoded_dict = {'TRISTEZZA':0, 'GIOIA':1, 'AMORE':2, 'RABBIA':3, 'PAURA':4, 'SORPRESA':5, 'NEUTRA':6}\n\ndf_train['label'] = df_train['emotion'].apply(lambda x: encoded_dict[x])\ndf_test['label'] = df_test['emotion'].apply(lambda x: encoded_dict[x])\ndf_val['label'] = df_val['emotion'].apply(lambda x: encoded_dict[x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def label2id(label):\n    if isinstance(label, list):\n        return [encoded_dict[label] for label in label]\n    else:\n        return encoded_dict[label]\n\ndef id2label(id):\n    encoded_dict_inv = {v: k for k, v in encoded_dict.items()}\n    \n    if isinstance(id, list):\n        return [encoded_dict_inv[i] for i in id]\n    else:\n        return encoded_dict_inv[id]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(df_train)\ntest_dataset = Dataset.from_pandas(df_test)\nval_dataset = Dataset.from_pandas(df_val)\n\n# Create the DatasetDict\ndataset = DatasetDict({'train': train_dataset, 'test': test_dataset, 'validation': val_dataset})\n\nprint(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize a dictionary to store updated datasets\nupdated_datasets = {}\n\n# Check for and remove duplicates in each split\nfor split in dataset.keys():\n    split_data = dataset[split]\n    \n    # Access the 'text' column within the list\n    text_column = split_data['text']\n    \n    # Initialize a set to track unique texts\n    unique_texts = set()\n    \n    # Initialize lists to store the filtered data\n    filtered_text = []\n    \n    # Iterate through the 'text' column and filter duplicates\n    for text in text_column:\n        if text not in unique_texts:\n            unique_texts.add(text)\n            filtered_text.append(text)\n    \n    # Create a new Dataset object with the filtered data\n    updated_datasets[split] = split_data.select(list(range(len(filtered_text))))\n    \n    # Print the number of removed duplicates\n    duplicate_count = len(text_column) - len(filtered_text)\n    print(f\"Duplicates removed in {split} split: {duplicate_count}\\n\")\n\n# Update the dataset dictionary with the filtered datasets\ndataset.update(updated_datasets)\n\n# Print the updated dataset information\nfor split in dataset.keys():\n    split_data = dataset[split]\n    print(f\"{split}: {len(split_data['text'])} rows\")\n\nprint(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Preprocess data","metadata":{}},{"cell_type":"code","source":"bert = TFAutoModel.from_pretrained('Twitter/twhin-bert-base', from_pt=True)\ntokenizer = AutoTokenizer.from_pretrained('Twitter/twhin-bert-base')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 256\n\ndef tokenize_text(dataset):\n    return tokenizer(\n        text=dataset['text'],\n        add_special_tokens=True,\n        return_token_type_ids=False,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_tensors='tf',\n        verbose=True\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_dataset = dataset.map(tokenize_text)\nencoded_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_dataset = encoded_dataset.remove_columns(['emotion','text'])\n\nencoded_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(encoded_dataset, data_type):\n    input_ids = np.array(encoded_dataset[data_type]['input_ids'])\n    input_ids = np.squeeze(input_ids, axis=1)\n\n    attention_mask = np.array(encoded_dataset[data_type]['attention_mask'])\n    attention_mask = np.squeeze(attention_mask, axis=1)\n\n    label = np.array(encoded_dataset[data_type]['label'])\n\n    return input_ids, attention_mask, label\n\ndef main_processing(encoded_dataset):\n    input_ids_train, attention_mask_train, label_train = preprocess_data(encoded_dataset, 'train')\n    input_ids_val, attention_mask_val, label_val = preprocess_data(encoded_dataset, 'validation')\n    input_ids_test, attention_mask_test, label_test = preprocess_data(encoded_dataset, 'test')\n\n    return input_ids_train, attention_mask_train, label_train, input_ids_val, attention_mask_val, label_val, input_ids_test, attention_mask_test, label_test\n\n# Usage\ninput_ids_train, attention_mask_train, label_train, input_ids_val, attention_mask_val, label_val, input_ids_test, attention_mask_test, label_test = main_processing(encoded_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Defining the model","metadata":{}},{"cell_type":"code","source":"input_ids = Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n\nembeddings = bert.bert(input_ids, attention_mask = input_mask)[0]\nout = GlobalMaxPooling1D(name=\"GlobalMaxPooling1d\")(embeddings)\nout = Dense(128, activation='relu',name=\"Dense_relu\")(out)\n\ny = Dense(7, activation='softmax')(out)\n    \nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\nmodel.layers[2].trainable = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(\n    learning_rate=1e-5,\n    epsilon=1e-08,\n    weight_decay=0.01,\n    name=\"AdamW\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, layer in enumerate(model.layers):\n    print(f\"Layer {i}: {layer.name}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = SparseCategoricalCrossentropy(\n    from_logits=False,\n    ignore_class=None,\n    reduction=\"auto\",\n    name=\"sparse_categorical_crossentropy\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scheduler(epoch,lr):\n    if epoch <2:\n        return lr\n    else:\n        return lr*tf.math.exp(-0.1)\n    \nlr_scheduler = LearningRateScheduler(scheduler)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stop = EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0,\n    patience=4,\n    verbose=0,\n    mode=\"auto\",\n    baseline=None,\n    restore_best_weights=True,\n    start_from_epoch=0,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer=optimizer,\n    loss=loss,\n    metrics=[\"sparse_categorical_accuracy\"]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Training","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n    x = {'input_ids':input_ids_train, 'attention_mask':attention_mask_train},\n    y = label_train,\n    validation_data = ({'input_ids':input_ids_val, 'attention_mask':attention_mask_val},\n                      (label_val)),\n    epochs=10,\n    batch_size=16,\n    callbacks=[lr_scheduler, early_stop]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='train_loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Metrics","metadata":{}},{"cell_type":"code","source":"predicted = model.predict({'input_ids': input_ids_test, 'attention_mask': attention_mask_test})\npredicted_labels = np.argmax(predicted, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_labels = predicted_labels.tolist()\npredicted_labels = id2label(predicted_labels)\npredicted_labels[0:7]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_test = label_test.tolist()\nlabel_test = id2label(label_test)\nlabel_test[0:7]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(label_test, predicted_labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(label_test, predicted_labels, labels =['NEUTRA', 'GIOIA', 'RABBIA', 'AMORE', 'PAURA', 'TRISTEZZA', 'SORPRESA'])\n\n# Create a heatmap for the confusion matrix\nplt.figure(figsize=(8, 6))\n\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['NEUTRA', 'GIOIA', 'RABBIA', 'AMORE', 'PAURA', 'TRISTEZZA', 'SORPRESA'], \n            yticklabels=['NEUTRA', 'GIOIA', 'RABBIA', 'AMORE', 'PAURA', 'TRISTEZZA', 'SORPRESA'])\n\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.1 Accuracy Score","metadata":{}},{"cell_type":"code","source":"accuracy = accuracy_score(label_test, predicted_labels) # (TP+TN)/P+N i.e total number of corrected classified tweet over total number of tweets\n\nprint(accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Precision Score","metadata":{}},{"cell_type":"code","source":"precision = precision_score(label_test, predicted_labels,average=None, labels=['TRISTEZZA','GIOIA','AMORE','RABBIA','PAURA','SORPRESA','NEUTRA']) # TP/(TP+FP) i.e if predicted a certain class, which is the probability of being really that class?\n\nprint(precision)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Recall (sensitivity) Score","metadata":{}},{"cell_type":"code","source":"recall = recall_score(label_test, predicted_labels,average=None, labels=['TRISTEZZA','GIOIA','AMORE','RABBIA','PAURA','SORPRESA','NEUTRA']) # TP/(TP+FN) i.e the ability of the estimator to predict all the tweets of a given class\n\nprint(recall)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.4 F1 Score","metadata":{}},{"cell_type":"code","source":"f1score = f1_score(label_test, predicted_labels,average=None, labels=['TRISTEZZA','GIOIA','AMORE','RABBIA','PAURA','SORPRESA','NEUTRA']) # 2*(precision*recall)/(precision+recall)\n\nprint(f1score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. To Hub","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import push_to_hub_keras\n\npush_to_hub_keras(model, 'FedeBerto/Griffith-Emotion')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import from_pretrained_keras\n\nmodel = from_pretrained_keras('FedeBerto/Griffith-Emotion')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
