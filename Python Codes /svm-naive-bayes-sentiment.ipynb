{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:34.275194Z","iopub.execute_input":"2023-09-20T09:45:34.275910Z","iopub.status.idle":"2023-09-20T09:45:34.287487Z","shell.execute_reply.started":"2023-09-20T09:45:34.275864Z","shell.execute_reply":"2023-09-20T09:45:34.286363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\n# Filter out the specific UserWarnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"unable to load libtensorflow_io_plugins.so\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"file system plugins are not loaded\")","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:34.288806Z","iopub.execute_input":"2023-09-20T09:45:34.289143Z","iopub.status.idle":"2023-09-20T09:45:34.300245Z","shell.execute_reply.started":"2023-09-20T09:45:34.289109Z","shell.execute_reply":"2023-09-20T09:45:34.299245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy metrics from Scikit-Learn\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:34.301684Z","iopub.execute_input":"2023-09-20T09:45:34.302019Z","iopub.status.idle":"2023-09-20T09:45:34.792195Z","shell.execute_reply.started":"2023-09-20T09:45:34.301986Z","shell.execute_reply":"2023-09-20T09:45:34.791196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hugging Face library\nfrom datasets import Dataset, DatasetDict","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:34.795270Z","iopub.execute_input":"2023-09-20T09:45:34.796182Z","iopub.status.idle":"2023-09-20T09:45:35.296358Z","shell.execute_reply.started":"2023-09-20T09:45:34.796146Z","shell.execute_reply":"2023-09-20T09:45:35.295379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NLP libraries\nimport re\nimport nltk\nimport simplemma\n\nfrom simplemma import text_lemmatizer\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:35.297713Z","iopub.execute_input":"2023-09-20T09:45:35.298355Z","iopub.status.idle":"2023-09-20T09:45:35.489572Z","shell.execute_reply.started":"2023-09-20T09:45:35.298291Z","shell.execute_reply":"2023-09-20T09:45:35.488628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Load Dataset","metadata":{}},{"cell_type":"code","source":"# Create a function to import the data from csv format\ndef load_data(file_path):\n    return pd.read_csv(file_path, header=None, delimiter='\\t', names=['sentiment', 'text'])\n\n\ntrain_path = '/kaggle/input/sentiment/train_bal_vdg_27_11.tsv'\ntest_path = '/kaggle/input/sentiment/test_bal_vdg_27_11.tsv'\nval_path = '/kaggle/input/sentiment/valid_bal_vdg_27_11.tsv'\n\ndf_train = load_data(train_path)\ndf_test = load_data(test_path)\ndf_val = load_data(val_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:35.490875Z","iopub.execute_input":"2023-09-20T09:45:35.491331Z","iopub.status.idle":"2023-09-20T09:45:35.538344Z","shell.execute_reply.started":"2023-09-20T09:45:35.491279Z","shell.execute_reply":"2023-09-20T09:45:35.537465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def converter(df): \n    mapping = {'NEG':'negative', 'NEU':'neutral', 'POS':'positive'} \n    df['sentiment'] = df['sentiment'].replace(mapping) \n    return df\n\ndf_train = converter(df_train) \ndf_val = converter(df_val) \ndf_test = converter(df_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:35.540331Z","iopub.execute_input":"2023-09-20T09:45:35.540873Z","iopub.status.idle":"2023-09-20T09:45:35.550789Z","shell.execute_reply.started":"2023-09-20T09:45:35.540840Z","shell.execute_reply":"2023-09-20T09:45:35.549849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To get an idea of the data\npd.set_option('display.max_colwidth', 150)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:35.552298Z","iopub.execute_input":"2023-09-20T09:45:35.552708Z","iopub.status.idle":"2023-09-20T09:45:35.567513Z","shell.execute_reply.started":"2023-09-20T09:45:35.552654Z","shell.execute_reply":"2023-09-20T09:45:35.566384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove user mention here. could not do it in the preprocess function\ndf_train['text'] = df_train['text'].str.replace('@[A-Za-z0-9]+\\s?', '', regex=True)\ndf_val['text'] = df_val['text'].str.replace('@[A-Za-z0-9]+\\s?', '', regex=True)\ndf_test['text'] = df_test['text'].str.replace('@[A-Za-z0-9]+\\s?', '', regex=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:35.569296Z","iopub.execute_input":"2023-09-20T09:45:35.569682Z","iopub.status.idle":"2023-09-20T09:45:35.579414Z","shell.execute_reply.started":"2023-09-20T09:45:35.569648Z","shell.execute_reply":"2023-09-20T09:45:35.578332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I'm combining the pandas dataframe to the dataset dictionary of Hugging Face\n\ntrain_dataset = Dataset.from_pandas(df_train)\ntest_dataset = Dataset.from_pandas(df_test)\nval_dataset = Dataset.from_pandas(df_val)\n\n# Create the DatasetDict\ndataset = DatasetDict({'train': train_dataset, 'test': test_dataset, 'validation': val_dataset})\n\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:35.581378Z","iopub.execute_input":"2023-09-20T09:45:35.582286Z","iopub.status.idle":"2023-09-20T09:45:35.607717Z","shell.execute_reply.started":"2023-09-20T09:45:35.582209Z","shell.execute_reply":"2023-09-20T09:45:35.606379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing duplicates\n\n# Initialize a dictionary to store updated datasets\nupdated_datasets = {}\n\n# Check for and remove duplicates in each split\nfor split in dataset.keys():\n    split_data = dataset[split]\n    \n    # Access the 'text' column within the list\n    text_column = split_data['text']\n    \n    # Initialize a set to track unique texts\n    unique_texts = set()\n    \n    # Initialize lists to store the filtered data\n    filtered_text = []\n    \n    # Iterate through the 'text' column and filter duplicates\n    for text in text_column:\n        if text not in unique_texts:\n            unique_texts.add(text)\n            filtered_text.append(text)\n    \n    # Create a new Dataset object with the filtered data\n    updated_datasets[split] = split_data.select(list(range(len(filtered_text))))\n    \n    # Print the number of removed duplicates\n    duplicate_count = len(text_column) - len(filtered_text)\n    print(f\"Duplicates removed in {split} split: {duplicate_count}\\n\")\n\n# Update the dataset dictionary with the filtered datasets\ndataset.update(updated_datasets)\n\n# Print the updated dataset information\nfor split in dataset.keys():\n    split_data = dataset[split]\n    print(f\"{split}: {len(split_data['text'])} rows\")\n\nprint(dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Prepocessing","metadata":{}},{"cell_type":"code","source":"italian_stopwords = set(stopwords.words('italian'))\n\n# Define a function to preprocess text\ndef preprocess_text(text):    \n    # Tokenization, lemmatization, removing punctuation, stopwords and URLs\n    text = text_lemmatizer(text, lang='it')\n    text = ' '.join(text)\n    \n    text = re.sub(r'[^\\w\\s\\']', '', text)\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    text = ' '.join(word for word in text.split() if word.lower() not in italian_stopwords)\n    \n    return text\n\n\n\n\ndef preprocess_dataset(dataset):\n    dataset['text'] = preprocess_text(dataset['text'])\n    return dataset\n\ndataset = dataset.map(preprocess_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:35.610142Z","iopub.execute_input":"2023-09-20T09:45:35.611256Z","iopub.status.idle":"2023-09-20T09:45:36.360435Z","shell.execute_reply.started":"2023-09-20T09:45:35.611221Z","shell.execute_reply":"2023-09-20T09:45:36.357459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['train']['text'][0:4]","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:36.363798Z","iopub.execute_input":"2023-09-20T09:45:36.364729Z","iopub.status.idle":"2023-09-20T09:45:36.374331Z","shell.execute_reply.started":"2023-09-20T09:45:36.364683Z","shell.execute_reply":"2023-09-20T09:45:36.373297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"## 3.1 TF-IDF ","metadata":{}},{"cell_type":"code","source":"# Convert the dataset to be ready for vectorization\nX_train = np.array(dataset['train']['text'])\nY_train = np.array(dataset['train']['sentiment'])\n\nX_val = np.array(dataset['validation']['text'])\nY_val = np.array(dataset['validation']['sentiment'])\n\nX_test = np.array(dataset['test']['text'])\nY_test = np.array(dataset['test']['sentiment'])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:36.385066Z","iopub.execute_input":"2023-09-20T09:45:36.386907Z","iopub.status.idle":"2023-09-20T09:45:36.421456Z","shell.execute_reply.started":"2023-09-20T09:45:36.386873Z","shell.execute_reply":"2023-09-20T09:45:36.420492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\ntf_x_train = vectorizer.fit_transform(X_train)\ntf_x_test = vectorizer.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:36.425908Z","iopub.execute_input":"2023-09-20T09:45:36.429778Z","iopub.status.idle":"2023-09-20T09:45:36.482387Z","shell.execute_reply.started":"2023-09-20T09:45:36.429742Z","shell.execute_reply":"2023-09-20T09:45:36.481418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Support Vector Machine","metadata":{}},{"cell_type":"code","source":"# LinearSVC\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:36.484000Z","iopub.execute_input":"2023-09-20T09:45:36.484369Z","iopub.status.idle":"2023-09-20T09:45:36.490050Z","shell.execute_reply.started":"2023-09-20T09:45:36.484334Z","shell.execute_reply":"2023-09-20T09:45:36.488573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm.fit(tf_x_train,Y_train)\n\ny_test_svm=svm.predict(tf_x_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:36.492099Z","iopub.execute_input":"2023-09-20T09:45:36.492993Z","iopub.status.idle":"2023-09-20T09:45:36.685978Z","shell.execute_reply.started":"2023-09-20T09:45:36.492959Z","shell.execute_reply":"2023-09-20T09:45:36.684844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:36.688128Z","iopub.execute_input":"2023-09-20T09:45:36.688744Z","iopub.status.idle":"2023-09-20T09:45:36.694797Z","shell.execute_reply.started":"2023-09-20T09:45:36.688708Z","shell.execute_reply":"2023-09-20T09:45:36.693715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb.fit(tf_x_train,Y_train)\n\ny_test_nb=nb.predict(tf_x_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:36.696074Z","iopub.execute_input":"2023-09-20T09:45:36.697071Z","iopub.status.idle":"2023-09-20T09:45:36.709986Z","shell.execute_reply.started":"2023-09-20T09:45:36.697038Z","shell.execute_reply":"2023-09-20T09:45:36.708862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Metrics","metadata":{}},{"cell_type":"code","source":"report_svm = classification_report(Y_test, y_test_svm)\n\nreport_nb = classification_report(Y_test, y_test_nb)\n\nprint(\"Support Vector Machine Classification Report:\")\nprint(report_svm)\n\nprint(\"\\nNaive Bayes Classification Report:\")\nprint(report_nb)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:36.711641Z","iopub.execute_input":"2023-09-20T09:45:36.712181Z","iopub.status.idle":"2023-09-20T09:45:36.738633Z","shell.execute_reply.started":"2023-09-20T09:45:36.712147Z","shell.execute_reply":"2023-09-20T09:45:36.737575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_svm = accuracy_score(Y_test, y_test_svm) # (TP+TN)/P+N i.e total number of corrected classified tweet over total number of tweets\n\naccuracy_nb = accuracy_score(Y_test, y_test_nb)\n\nprint(\"Support Vector Machine accuracy:\", accuracy_svm)\nprint(\"Naive Bayes accuracy:\", accuracy_nb)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:36.740006Z","iopub.execute_input":"2023-09-20T09:45:36.740470Z","iopub.status.idle":"2023-09-20T09:45:36.749515Z","shell.execute_reply.started":"2023-09-20T09:45:36.740436Z","shell.execute_reply":"2023-09-20T09:45:36.748370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision_svm = precision_score(Y_test, y_test_svm,average=None, labels=['negative','neutral','positive']) # TP/(TP+FP) i.e if predicted a certain class, which is the probability of being really that class?\n\nprecision_nb = precision_score(Y_test, y_test_nb,average=None, labels=['negative','neutral','positive'])\n\nprint(\"Support Vector Machine precision:\", precision_svm)\nprint(\"Naive Bayes precision:\", precision_nb)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:36.751586Z","iopub.execute_input":"2023-09-20T09:45:36.752392Z","iopub.status.idle":"2023-09-20T09:45:36.768163Z","shell.execute_reply.started":"2023-09-20T09:45:36.752356Z","shell.execute_reply":"2023-09-20T09:45:36.766857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recall_svm = recall_score(Y_test, y_test_svm,average=None, labels=['negative','neutral','positive']) # TP/(TP+FN) i.e the ability of the estimator to predict all the tweets of a given class\n\nrecall_nb = recall_score(Y_test, y_test_nb,average=None, labels=['negative','neutral','positive'])\n\n\nprint(\"Support Vector Machine recall:\", recall_svm)\nprint(\"Naive Bayes recall:\", recall_nb)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:36.770179Z","iopub.execute_input":"2023-09-20T09:45:36.771028Z","iopub.status.idle":"2023-09-20T09:45:36.786345Z","shell.execute_reply.started":"2023-09-20T09:45:36.770993Z","shell.execute_reply":"2023-09-20T09:45:36.785367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1score_svm = f1_score(Y_test, y_test_svm,average=None, labels=['negative','neutral','positive']) # 2*(precision*recall)/(precision+recall)\n\nf1score_nb = f1_score(Y_test, y_test_nb,average=None, labels=['negative','neutral','positive'])\n\n\nprint(\"Support Vector Machine f1-score:\", f1score_svm)\nprint(\"Naive Bayes f1-score:\", f1score_nb)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:45:36.787367Z","iopub.execute_input":"2023-09-20T09:45:36.787637Z","iopub.status.idle":"2023-09-20T09:45:36.803501Z","shell.execute_reply.started":"2023-09-20T09:45:36.787612Z","shell.execute_reply":"2023-09-20T09:45:36.802535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
