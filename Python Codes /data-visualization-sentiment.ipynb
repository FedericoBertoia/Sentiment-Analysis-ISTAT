{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:06:33.977398Z","iopub.execute_input":"2023-09-17T17:06:33.978690Z","iopub.status.idle":"2023-09-17T17:06:34.288547Z","shell.execute_reply.started":"2023-09-17T17:06:33.978641Z","shell.execute_reply":"2023-09-17T17:06:34.287015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\n# Filter out the specific UserWarnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"unable to load libtensorflow_io_plugins.so\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"file system plugins are not loaded\")","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:06:34.290064Z","iopub.execute_input":"2023-09-17T17:06:34.290587Z","iopub.status.idle":"2023-09-17T17:06:34.296944Z","shell.execute_reply.started":"2023-09-17T17:06:34.290551Z","shell.execute_reply":"2023-09-17T17:06:34.295593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hugging Face library\nfrom transformers import AutoTokenizer, TFAutoModel","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:06:34.298405Z","iopub.execute_input":"2023-09-17T17:06:34.298852Z","iopub.status.idle":"2023-09-17T17:06:35.134292Z","shell.execute_reply.started":"2023-09-17T17:06:34.298787Z","shell.execute_reply":"2023-09-17T17:06:35.132764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hugging Face library\nfrom datasets import Dataset, DatasetDict","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:06:35.137774Z","iopub.execute_input":"2023-09-17T17:06:35.138374Z","iopub.status.idle":"2023-09-17T17:06:35.596621Z","shell.execute_reply.started":"2023-09-17T17:06:35.138339Z","shell.execute_reply":"2023-09-17T17:06:35.595492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:06:35.597880Z","iopub.execute_input":"2023-09-17T17:06:35.598405Z","iopub.status.idle":"2023-09-17T17:06:36.110168Z","shell.execute_reply.started":"2023-09-17T17:06:35.598376Z","shell.execute_reply":"2023-09-17T17:06:36.108510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Load Datasets","metadata":{}},{"cell_type":"code","source":"# Create a function to import the data from csv format\ndef load_data(file_path):\n    return pd.read_csv(file_path, header=None, delimiter='\\t', names=['sentiment', 'text'])\n\n\ntrain_path = '/kaggle/input/sentiment/train_bal_vdg_27_11.tsv'\ntest_path = '/kaggle/input/sentiment/test_bal_vdg_27_11.tsv'\nval_path = '/kaggle/input/sentiment/valid_bal_vdg_27_11.tsv'\n\ndf_train = load_data(train_path)\ndf_test = load_data(test_path)\ndf_val = load_data(val_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:06:36.111940Z","iopub.execute_input":"2023-09-17T17:06:36.112270Z","iopub.status.idle":"2023-09-17T17:06:36.139907Z","shell.execute_reply.started":"2023-09-17T17:06:36.112234Z","shell.execute_reply":"2023-09-17T17:06:36.138286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To get an idea of the data\npd.set_option('display.max_colwidth', 150)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:06:36.141453Z","iopub.execute_input":"2023-09-17T17:06:36.141739Z","iopub.status.idle":"2023-09-17T17:06:36.154790Z","shell.execute_reply.started":"2023-09-17T17:06:36.141714Z","shell.execute_reply":"2023-09-17T17:06:36.153467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I'm combining the pandas dataframe to the dataset dictionary of Hugging Face\n\ntrain_dataset = Dataset.from_pandas(df_train)\ntest_dataset = Dataset.from_pandas(df_test)\nval_dataset = Dataset.from_pandas(df_val)\n\n# Create the DatasetDict\ndataset = DatasetDict({'train': train_dataset, 'test': test_dataset, 'validation': val_dataset})\n\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:06:36.155720Z","iopub.execute_input":"2023-09-17T17:06:36.156004Z","iopub.status.idle":"2023-09-17T17:06:36.180423Z","shell.execute_reply.started":"2023-09-17T17:06:36.155981Z","shell.execute_reply":"2023-09-17T17:06:36.179271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.5 Check Duplicates","metadata":{}},{"cell_type":"code","source":"# Initialize a dictionary to store updated datasets\nupdated_datasets = {}\n\n# Check for and remove duplicates in each split\nfor split in dataset.keys():\n    split_data = dataset[split]\n    \n    # Access the 'text' column within the list\n    text_column = split_data['text']\n    \n    # Initialize a set to track unique texts\n    unique_texts = set()\n    \n    # Initialize lists to store the filtered data\n    filtered_text = []\n    \n    # Iterate through the 'text' column and filter duplicates\n    for text in text_column:\n        if text not in unique_texts:\n            unique_texts.add(text)\n            filtered_text.append(text)\n    \n    # Create a new Dataset object with the filtered data\n    updated_datasets[split] = split_data.select(list(range(len(filtered_text))))\n    \n    # Print the number of removed duplicates\n    duplicate_count = len(text_column) - len(filtered_text)\n    print(f\"Duplicates removed in {split} split: {duplicate_count}\\n\")\n\n# Update the dataset dictionary with the filtered datasets\ndataset.update(updated_datasets)\n\n# Print the updated dataset information\nfor split in dataset.keys():\n    split_data = dataset[split]\n    print(f\"{split}: {len(split_data['text'])} rows\")\n\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:06:36.181568Z","iopub.execute_input":"2023-09-17T17:06:36.181896Z","iopub.status.idle":"2023-09-17T17:06:36.209466Z","shell.execute_reply.started":"2023-09-17T17:06:36.181869Z","shell.execute_reply":"2023-09-17T17:06:36.207446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Sentiment Distribution","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\n# Initialize dictionaries to store sentiment counts and categories for each split\nsentiment_counts = {}\nsentiment_categories = {}\n\n# Loop through each split\nfor split in dataset.keys():\n    split_data = dataset[split]\n    \n    # Extract sentiment data from the 'sentiment' column within the list\n    sentiment_data = [item for item in split_data['sentiment']]\n    \n    # Calculate sentiment counts for the current split using Counter\n    sentiment_count = Counter(sentiment_data)\n    sentiment_counts[split] = sentiment_count\n    \n    # Get unique sentiment categories for the current split\n    sentiment_category = list(sentiment_count.keys())\n    sentiment_categories[split] = sentiment_category\n\n# Combine all unique sentiment categories across all splits and sort them\nall_sentiment_categories = sorted(set().union(*sentiment_categories.values()))\n\n# Create subplots for each split\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Loop through each split and plot the sentiment counts\nfor i, split in enumerate(dataset.keys()):\n    x = np.array(all_sentiment_categories)\n    y = np.array([sentiment_counts[split].get(category, 0) for category in x])\n    axs[i].bar(x, y)\n    \n    axs[i].set_title(f\"Sentiment Distribution ({split} split)\")\n    axs[i].set_xlabel(\"Sentiment category\")\n    axs[i].set_ylabel(\"Number of tweets\")\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:06:37.893426Z","iopub.execute_input":"2023-09-17T17:06:37.893753Z","iopub.status.idle":"2023-09-17T17:06:38.520405Z","shell.execute_reply.started":"2023-09-17T17:06:37.893726Z","shell.execute_reply":"2023-09-17T17:06:38.518854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. WordCloud","metadata":{}},{"cell_type":"code","source":"!python -m spacy download it_core_news_md","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:06:39.055619Z","iopub.execute_input":"2023-09-17T17:06:39.056045Z","iopub.status.idle":"2023-09-17T17:06:56.174322Z","shell.execute_reply.started":"2023-09-17T17:06:39.056018Z","shell.execute_reply":"2023-09-17T17:06:56.173214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\n\n# Load the Italian language model\nnlp = spacy.load(\"it_core_news_md\")","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:06:56.176161Z","iopub.execute_input":"2023-09-17T17:06:56.176477Z","iopub.status.idle":"2023-09-17T17:07:01.708024Z","shell.execute_reply.started":"2023-09-17T17:06:56.176451Z","shell.execute_reply":"2023-09-17T17:07:01.706999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nitalian_stopwords = set(stopwords.words('italian'))\n\nfrom nltk.tokenize import word_tokenize \n\n# Define a function to preprocess text\ndef preprocess_text(text):\n    # Remove punctuation, URLs, and user mentions\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'@\\w+', '', text)\n    \n    # Process text using spaCy\n    doc = nlp(text)\n    \n    # Filter out only nouns and lemmatize them\n    lemmatized_nouns = [token.lemma_ for token in doc  if token.pos_ == 'NOUN']\n    \n    # Remove stopwords\n    lemmatized_nouns = [word for word in lemmatized_nouns if word.lower() not in italian_stopwords]\n    \n    # Join the filtered and lemmatized nouns into a string\n    text = ' '.join(lemmatized_nouns)\n    \n    return text\n\n\n\n\n\ndef preprocess_dataset(dataset):\n    dataset['text'] = preprocess_text(dataset['text'])\n    return dataset\n\ndataset = dataset.map(preprocess_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:20:42.825762Z","iopub.execute_input":"2023-09-17T17:20:42.826163Z","iopub.status.idle":"2023-09-17T17:20:50.578342Z","shell.execute_reply.started":"2023-09-17T17:20:42.826111Z","shell.execute_reply":"2023-09-17T17:20:50.576682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:20:54.323080Z","iopub.execute_input":"2023-09-17T17:20:54.323473Z","iopub.status.idle":"2023-09-17T17:20:54.330513Z","shell.execute_reply.started":"2023-09-17T17:20:54.323444Z","shell.execute_reply":"2023-09-17T17:20:54.328445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a WordCloud object\n\n# Combine the text from 'train', 'test', and 'validation' splits\ncombined_text = []\n\nfor split in ['train', 'test', 'validation']:\n    combined_text.extend([text for text in dataset[split]['text']])\n\n# Concatenate the combined text into a single string\ntext = ' '.join(combined_text)\n\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n\n# Display the word cloud using matplotlib\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")  # Turn off axis labels\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:20:55.860619Z","iopub.execute_input":"2023-09-17T17:20:55.860976Z","iopub.status.idle":"2023-09-17T17:20:56.773901Z","shell.execute_reply.started":"2023-09-17T17:20:55.860949Z","shell.execute_reply":"2023-09-17T17:20:56.773016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Checking word distribution among classes","metadata":{}},{"cell_type":"code","source":"# Create empty lists for each class\nneg_text = []\nneu_text = []\npos_text = []\n\n\nfor split in dataset.keys():\n    for text, label in zip(dataset[split]['text'], dataset[split]['sentiment']):\n        # Check the label and append the text to the corresponding list\n        if label == 'NEG':\n            neg_text.append(text)\n        elif label == 'NEU':\n            neu_text.append(text)\n        elif label == 'POS':\n            pos_text.append(text)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:21:00.590787Z","iopub.execute_input":"2023-09-17T17:21:00.591770Z","iopub.status.idle":"2023-09-17T17:21:00.601195Z","shell.execute_reply.started":"2023-09-17T17:21:00.591739Z","shell.execute_reply":"2023-09-17T17:21:00.599379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All words contained in \"category tweets\"\nnegative = []\nneutral = []\npositive = []\n\n# Process negative text\nfor sentence in neg_text:\n    doc = nlp(sentence)\n    nouns = [token.text for token in doc if token.pos_ == 'NOUN']\n    negative.extend(nouns)\n\n# Process neutral text\nfor sentence in neu_text:\n    doc = nlp(sentence)\n    nouns = [token.text for token in doc if token.pos_ == 'NOUN']\n    neutral.extend(nouns)\n\n# Process positive text\nfor sentence in pos_text:\n    doc = nlp(sentence)\n    nouns = [token.text for token in doc if token.pos_ == 'NOUN']\n    positive.extend(nouns)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:21:01.070587Z","iopub.execute_input":"2023-09-17T17:21:01.070954Z","iopub.status.idle":"2023-09-17T17:21:06.339288Z","shell.execute_reply.started":"2023-09-17T17:21:01.070926Z","shell.execute_reply":"2023-09-17T17:21:06.337924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.probability import FreqDist\n\n\nfdist_neg = FreqDist(negative)\nfdist_neu = FreqDist(neutral)\nfdist_pos = FreqDist(positive)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:21:06.341460Z","iopub.execute_input":"2023-09-17T17:21:06.341815Z","iopub.status.idle":"2023-09-17T17:21:06.351529Z","shell.execute_reply.started":"2023-09-17T17:21:06.341770Z","shell.execute_reply":"2023-09-17T17:21:06.349925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the most common words for each category\ntop_words_neg = fdist_neg.most_common(15)\ntop_words_neu = fdist_neu.most_common(15)\ntop_words_pos = fdist_pos.most_common(15)\n\n# Unzip the top words and frequencies\ntop_words_neg, frequencies_neg = zip(*top_words_neg)\ntop_words_neu, frequencies_neu = zip(*top_words_neu)\ntop_words_pos, frequencies_pos = zip(*top_words_pos)\n\n# Create subplots\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Plot for negative words\naxes[0].barh(top_words_neg, frequencies_neg)\naxes[0].set_xlabel('Frequency')\naxes[0].set_ylabel('Words')\naxes[0].set_title('Top 5 Negative Words')\naxes[0].invert_yaxis()\n\n# Plot for neutral words\naxes[1].barh(top_words_neu, frequencies_neu)\naxes[1].set_xlabel('Frequency')\naxes[1].set_ylabel('Words')\naxes[1].set_title('Top 5 Neutral Words')\naxes[1].invert_yaxis()\n\n# Plot for positive words\naxes[2].barh(top_words_pos, frequencies_pos)\naxes[2].set_xlabel('Frequency')\naxes[2].set_ylabel('Words')\naxes[2].set_title('Top 5 Positive Words')\naxes[2].invert_yaxis()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:21:06.353058Z","iopub.execute_input":"2023-09-17T17:21:06.353397Z","iopub.status.idle":"2023-09-17T17:21:07.048388Z","shell.execute_reply.started":"2023-09-17T17:21:06.353369Z","shell.execute_reply":"2023-09-17T17:21:07.047081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Similarity Index","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Create a TF-IDF vectorizer and fit it on all tweets\ntfidf_vectorizer = TfidfVectorizer()\nall_tweets = negative + neutral + positive\ntfidf_matrix = tfidf_vectorizer.fit_transform(all_tweets)\n\n# Calculate the cosine similarity between TF-IDF vectors\ncosine_neg_neu = cosine_similarity(tfidf_matrix[:len(negative)], tfidf_matrix[len(negative):len(negative) + len(neutral)])\ncosine_neg_pos = cosine_similarity(tfidf_matrix[:len(negative)], tfidf_matrix[len(negative) + len(neutral):])\ncosine_neu_pos = cosine_similarity(tfidf_matrix[len(negative):len(negative) + len(neutral)], tfidf_matrix[len(negative) + len(neutral):])\n\navg_cosine_neg_neu = np.mean(cosine_neg_neu)\navg_cosine_neg_pos = np.mean(cosine_neg_pos)\navg_cosine_neu_pos = np.mean(cosine_neu_pos)\n\nprint(\"Average Cosine Similarity:\")\nprint(\"NEG vs. NEU:\", avg_cosine_neg_neu)\nprint(\"NEG vs. POS:\", avg_cosine_neg_pos)\nprint(\"NEU vs. POS:\", avg_cosine_neu_pos)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:19:39.098632Z","iopub.execute_input":"2023-09-17T17:19:39.100013Z","iopub.status.idle":"2023-09-17T17:19:39.513357Z","shell.execute_reply.started":"2023-09-17T17:19:39.099956Z","shell.execute_reply":"2023-09-17T17:19:39.512184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
