{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:15.268953Z","iopub.execute_input":"2023-09-20T09:53:15.269376Z","iopub.status.idle":"2023-09-20T09:53:15.279238Z","shell.execute_reply.started":"2023-09-20T09:53:15.269346Z","shell.execute_reply":"2023-09-20T09:53:15.278248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\n# Filter out the specific UserWarnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"unable to load libtensorflow_io_plugins.so\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"file system plugins are not loaded\")","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:15.285363Z","iopub.execute_input":"2023-09-20T09:53:15.286321Z","iopub.status.idle":"2023-09-20T09:53:15.293647Z","shell.execute_reply.started":"2023-09-20T09:53:15.286286Z","shell.execute_reply":"2023-09-20T09:53:15.292816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy metrics from Scikit-Learn\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:15.294846Z","iopub.execute_input":"2023-09-20T09:53:15.295394Z","iopub.status.idle":"2023-09-20T09:53:15.785312Z","shell.execute_reply.started":"2023-09-20T09:53:15.295323Z","shell.execute_reply":"2023-09-20T09:53:15.784353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hugging Face library\nfrom datasets import Dataset, DatasetDict","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:15.788726Z","iopub.execute_input":"2023-09-20T09:53:15.789264Z","iopub.status.idle":"2023-09-20T09:53:16.270549Z","shell.execute_reply.started":"2023-09-20T09:53:15.789229Z","shell.execute_reply":"2023-09-20T09:53:16.269426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NLP libraries\nimport re\nimport nltk\nimport simplemma\n\nfrom simplemma import text_lemmatizer\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:16.272340Z","iopub.execute_input":"2023-09-20T09:53:16.273372Z","iopub.status.idle":"2023-09-20T09:53:16.483101Z","shell.execute_reply.started":"2023-09-20T09:53:16.273330Z","shell.execute_reply":"2023-09-20T09:53:16.482023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Load Dataset","metadata":{}},{"cell_type":"code","source":"def load_data(file_path):\n    return pd.read_csv(file_path, header=None, delimiter='\\t', names = ['emotion','text'])\n\ntrain_path = '/kaggle/input/emotion/train-emotion-all.tsv'\ntest_path = '/kaggle/input/emotion/test-emotion-all.tsv'\nval_path = '/kaggle/input/emotion/valid-emotion-all.tsv'\n\ndf_train = load_data(train_path)\ndf_test = load_data(test_path)\ndf_val = load_data(val_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:16.484559Z","iopub.execute_input":"2023-09-20T09:53:16.485627Z","iopub.status.idle":"2023-09-20T09:53:16.509849Z","shell.execute_reply.started":"2023-09-20T09:53:16.485588Z","shell.execute_reply":"2023-09-20T09:53:16.508896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To get an idea of the data\npd.set_option('display.max_colwidth', 150)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:16.512896Z","iopub.execute_input":"2023-09-20T09:53:16.513205Z","iopub.status.idle":"2023-09-20T09:53:16.527180Z","shell.execute_reply.started":"2023-09-20T09:53:16.513179Z","shell.execute_reply":"2023-09-20T09:53:16.526138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove user mention here. could not do it in the preprocess function\ndf_train['text'] = df_train['text'].str.replace('@[A-Za-z0-9]+\\s?', '', regex=True)\ndf_val['text'] = df_val['text'].str.replace('@[A-Za-z0-9]+\\s?', '', regex=True)\ndf_test['text'] = df_test['text'].str.replace('@[A-Za-z0-9]+\\s?', '', regex=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:16.529639Z","iopub.execute_input":"2023-09-20T09:53:16.530349Z","iopub.status.idle":"2023-09-20T09:53:16.542926Z","shell.execute_reply.started":"2023-09-20T09:53:16.530312Z","shell.execute_reply":"2023-09-20T09:53:16.541895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I'm combining the pandas dataframe to the dataset dictionary of Hugging Face\n\ntrain_dataset = Dataset.from_pandas(df_train)\ntest_dataset = Dataset.from_pandas(df_test)\nval_dataset = Dataset.from_pandas(df_val)\n\n# Create the DatasetDict\ndataset = DatasetDict({'train': train_dataset, 'test': test_dataset, 'validation': val_dataset})\n\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:16.545283Z","iopub.execute_input":"2023-09-20T09:53:16.545941Z","iopub.status.idle":"2023-09-20T09:53:16.575548Z","shell.execute_reply.started":"2023-09-20T09:53:16.545909Z","shell.execute_reply":"2023-09-20T09:53:16.574536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing duplicates\n\n# Initialize a dictionary to store updated datasets\nupdated_datasets = {}\n\n# Check for and remove duplicates in each split\nfor split in dataset.keys():\n    split_data = dataset[split]\n    \n    # Access the 'text' column within the list\n    text_column = split_data['text']\n    \n    # Initialize a set to track unique texts\n    unique_texts = set()\n    \n    # Initialize lists to store the filtered data\n    filtered_text = []\n    \n    # Iterate through the 'text' column and filter duplicates\n    for text in text_column:\n        if text not in unique_texts:\n            unique_texts.add(text)\n            filtered_text.append(text)\n    \n    # Create a new Dataset object with the filtered data\n    updated_datasets[split] = split_data.select(list(range(len(filtered_text))))\n    \n    # Print the number of removed duplicates\n    duplicate_count = len(text_column) - len(filtered_text)\n    print(f\"Duplicates removed in {split} split: {duplicate_count}\\n\")\n\n# Update the dataset dictionary with the filtered datasets\ndataset.update(updated_datasets)\n\n# Print the updated dataset information\nfor split in dataset.keys():\n    split_data = dataset[split]\n    print(f\"{split}: {len(split_data['text'])} rows\")\n\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:16.576885Z","iopub.execute_input":"2023-09-20T09:53:16.577175Z","iopub.status.idle":"2023-09-20T09:53:16.627699Z","shell.execute_reply.started":"2023-09-20T09:53:16.577147Z","shell.execute_reply":"2023-09-20T09:53:16.626695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Prepocessing","metadata":{}},{"cell_type":"code","source":"italian_stopwords = set(stopwords.words('italian'))\n\n# Define a function to preprocess text\ndef preprocess_text(text):    \n    # Tokenization, lemmatization, removing punctuation, stopwords and URLs\n    text = text_lemmatizer(text, lang='it')\n    text = ' '.join(text)\n    \n    text = re.sub(r'[^\\w\\s\\']', '', text)\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    text = ' '.join(word for word in text.split() if word.lower() not in italian_stopwords)\n    \n    return text\n\n\n\n\ndef preprocess_dataset(dataset):\n    dataset['text'] = preprocess_text(dataset['text'])\n    return dataset\n\ndataset = dataset.map(preprocess_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:16.629264Z","iopub.execute_input":"2023-09-20T09:53:16.629608Z","iopub.status.idle":"2023-09-20T09:53:17.464533Z","shell.execute_reply.started":"2023-09-20T09:53:16.629575Z","shell.execute_reply":"2023-09-20T09:53:17.463556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"## 3.1 TF-IDF ","metadata":{}},{"cell_type":"code","source":"# Convert the dataset to be ready for vectorization\nX_train = np.array(dataset['train']['text'])\nY_train = np.array(dataset['train']['emotion'])\n\nX_val = np.array(dataset['validation']['text'])\nY_val = np.array(dataset['validation']['emotion'])\n\nX_test = np.array(dataset['test']['text'])\nY_test = np.array(dataset['test']['emotion'])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:17.466105Z","iopub.execute_input":"2023-09-20T09:53:17.466709Z","iopub.status.idle":"2023-09-20T09:53:17.486541Z","shell.execute_reply.started":"2023-09-20T09:53:17.466673Z","shell.execute_reply":"2023-09-20T09:53:17.485616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\nvectorizer = TfidfVectorizer()\ntf_x_train = vectorizer.fit_transform(X_train)\ntf_x_test = vectorizer.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:17.491263Z","iopub.execute_input":"2023-09-20T09:53:17.491564Z","iopub.status.idle":"2023-09-20T09:53:17.570468Z","shell.execute_reply.started":"2023-09-20T09:53:17.491535Z","shell.execute_reply":"2023-09-20T09:53:17.569533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Support Vector Machine","metadata":{}},{"cell_type":"code","source":"# LinearSVC\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:17.571953Z","iopub.execute_input":"2023-09-20T09:53:17.572341Z","iopub.status.idle":"2023-09-20T09:53:17.578572Z","shell.execute_reply.started":"2023-09-20T09:53:17.572303Z","shell.execute_reply":"2023-09-20T09:53:17.577528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm.fit(tf_x_train,Y_train)\n\ny_test_svm=svm.predict(tf_x_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:17.580146Z","iopub.execute_input":"2023-09-20T09:53:17.581053Z","iopub.status.idle":"2023-09-20T09:53:18.205366Z","shell.execute_reply.started":"2023-09-20T09:53:17.581028Z","shell.execute_reply":"2023-09-20T09:53:18.204384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:18.206810Z","iopub.execute_input":"2023-09-20T09:53:18.207140Z","iopub.status.idle":"2023-09-20T09:53:18.221761Z","shell.execute_reply.started":"2023-09-20T09:53:18.207108Z","shell.execute_reply":"2023-09-20T09:53:18.220906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb.fit(tf_x_train,Y_train)\n\ny_test_nb=nb.predict(tf_x_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:18.222927Z","iopub.execute_input":"2023-09-20T09:53:18.223276Z","iopub.status.idle":"2023-09-20T09:53:18.235791Z","shell.execute_reply.started":"2023-09-20T09:53:18.223243Z","shell.execute_reply":"2023-09-20T09:53:18.234858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Metrics","metadata":{}},{"cell_type":"code","source":"report_svm = classification_report(Y_test, y_test_svm)\n\nreport_nb = classification_report(Y_test, y_test_nb)\n\nprint(\"Support Vector Machine Classification Report:\")\nprint(report_svm)\n\nprint(\"\\nNaive Bayes Classification Report:\")\nprint(report_nb)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:18.237127Z","iopub.execute_input":"2023-09-20T09:53:18.237703Z","iopub.status.idle":"2023-09-20T09:53:18.275432Z","shell.execute_reply.started":"2023-09-20T09:53:18.237668Z","shell.execute_reply":"2023-09-20T09:53:18.274445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_svm = accuracy_score(Y_test, y_test_svm) # (TP+TN)/P+N i.e total number of corrected classified tweet over total number of tweets\n\naccuracy_nb = accuracy_score(Y_test, y_test_nb)\n\nprint(\"Support Vector Machine accuracy:\", accuracy_svm)\nprint(\"Naive Bayes accuracy:\", accuracy_nb)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:18.276969Z","iopub.execute_input":"2023-09-20T09:53:18.277981Z","iopub.status.idle":"2023-09-20T09:53:18.287015Z","shell.execute_reply.started":"2023-09-20T09:53:18.277949Z","shell.execute_reply":"2023-09-20T09:53:18.285771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision_svm = precision_score(Y_test, y_test_svm,average=None, labels=['TRISTEZZA','GIOIA','AMORE','RABBIA','PAURA','SORPRESA','NEUTRA']) # TP/(TP+FP) i.e if predicted a certain class, which is the probability of being really that class?\n\nprecision_nb = precision_score(Y_test, y_test_nb,average=None, labels=['TRISTEZZA','GIOIA','AMORE','RABBIA','PAURA','SORPRESA','NEUTRA'])\n\nprint(\"Support Vector Machine precision:\", precision_svm)\nprint(\"Naive Bayes precision:\", precision_nb)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:18.289034Z","iopub.execute_input":"2023-09-20T09:53:18.289942Z","iopub.status.idle":"2023-09-20T09:53:18.308143Z","shell.execute_reply.started":"2023-09-20T09:53:18.289908Z","shell.execute_reply":"2023-09-20T09:53:18.307183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recall_svm = recall_score(Y_test, y_test_svm,average=None, labels=['TRISTEZZA','GIOIA','AMORE','RABBIA','PAURA','SORPRESA','NEUTRA']) # TP/(TP+FN) i.e the ability of the estimator to predict all the tweets of a given class\n\nrecall_nb = recall_score(Y_test, y_test_nb,average=None, labels=['TRISTEZZA','GIOIA','AMORE','RABBIA','PAURA','SORPRESA','NEUTRA'])\n\n\nprint(\"Support Vector Machine recall:\", recall_svm)\nprint(\"Naive Bayes recall:\", recall_nb)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:18.309592Z","iopub.execute_input":"2023-09-20T09:53:18.310076Z","iopub.status.idle":"2023-09-20T09:53:18.327519Z","shell.execute_reply.started":"2023-09-20T09:53:18.310045Z","shell.execute_reply":"2023-09-20T09:53:18.326705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1score_svm = f1_score(Y_test, y_test_svm,average=None, labels=['TRISTEZZA','GIOIA','AMORE','RABBIA','PAURA','SORPRESA','NEUTRA']) # 2*(precision*recall)/(precision+recall)\n\nf1score_nb = f1_score(Y_test, y_test_nb,average=None, labels=['TRISTEZZA','GIOIA','AMORE','RABBIA','PAURA','SORPRESA','NEUTRA'])\n\n\nprint(\"Support Vector Machine f1-score:\", f1score_svm)\nprint(\"Naive Bayes f1-score:\", f1score_nb)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T09:53:18.328507Z","iopub.execute_input":"2023-09-20T09:53:18.328860Z","iopub.status.idle":"2023-09-20T09:53:18.346307Z","shell.execute_reply.started":"2023-09-20T09:53:18.328827Z","shell.execute_reply":"2023-09-20T09:53:18.345247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
